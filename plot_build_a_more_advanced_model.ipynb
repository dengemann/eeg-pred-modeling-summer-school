{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-prime",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import mne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-outdoors",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some viz config\n",
    "small_size, medium_size, bigger_size = 13, 18, 18\n",
    "\n",
    "plt.rc('font', size=small_size)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=small_size)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=medium_size)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=small_size)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=small_size)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=small_size)    # legend fontsize\n",
    "plt.rc('figure', titlesize=bigger_size)  # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-graduation",
   "metadata": {},
   "source": [
    "## Let's start all over again\n",
    "\n",
    "We will just read in the data as we did before, this time focusing on covariance matrices which, when combined with more advanced feature modeling, may improve performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-montreal",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "df_demographics = pd.read_csv('./inputs/Demographic_data.csv', header=1)\n",
    "\n",
    "# remove empty columns\n",
    "df_demographics = df_demographics.iloc[:, :5].set_index('Code')\n",
    "df_demographics\n",
    "\n",
    "\n",
    "# The later code uses the prefix \"sub-\" in the participants identifier, we will add it here to be fine\n",
    "\n",
    "df_demographics.index = \"sub-\" + df_demographics.index\n",
    "df_demographics.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-freight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we read in the processing log to see for which participants we have EEG\n",
    "\n",
    "proc_log = pd.read_csv('./outputs/autoreject_log.csv')\n",
    "good_subjects = proc_log.query('ok == \"OK\"').subject\n",
    "good_subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-departure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we filter the demographic list accordingly and establish the same order\n",
    "\n",
    "df_demographics = df_demographics.loc[good_subjects]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-detail",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can put some data aside for testing and focus on 80 percent of the cases for exploring\n",
    "\n",
    "train_cases, test_cases = train_test_split(df_demographics, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-combination",
   "metadata": {},
   "source": [
    "## Read the pre-computed features\n",
    "\n",
    "We first start with the power spectra. As we read, we make sure features are stored in\n",
    "the same order as our meta info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naked-covering",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = mne.externals.h5io.read_hdf5('./outputs/features_eyes-closed.h5')\n",
    "covs = [features[sub]['covs'] for sub in train_cases.index]\n",
    "X_covs = np.array(covs)\n",
    "print(X_covs.shape)  # hooray we have an array! :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outer-burns",
   "metadata": {},
   "outputs": [],
   "source": [
    "# But what are these dimensions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-timing",
   "metadata": {},
   "source": [
    "## Let's explore these covariances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honey-walker",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cases['age_group'] = pd.cut(train_cases.Age, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-breathing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can do a group by after setting the index to range\n",
    "train_cases = train_cases.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suffering-firewall",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "color = plt.cm.viridis(np.linspace(0.1, 0.9, 4))\n",
    "fig, axes = plt.subplots(1, 4, figsize=(14, 9))\n",
    "for ii, (key, inds) in enumerate(train_cases.groupby('age_group').groups.items()):\n",
    "    im = axes[ii].matshow(X_covs[inds][1].mean(0), cmap='RdBu')\n",
    "    axes[ii].set_title(f'Age in {key}')\n",
    "    divider = make_axes_locatable(axes[ii])\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.2)\n",
    "    fig.colorbar(im, cax=cax)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-student",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_counts = pd.read_csv('./outputs/channel_counts.csv')\n",
    "\n",
    "ch_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manufactured-night",
   "metadata": {},
   "source": [
    "What we see here suggests there is highly structured noise. This may be atypical. **If you see something like this please go back to your preprocessing work and and double check that all is as expected.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-statistics",
   "metadata": {},
   "source": [
    "# A more advanced model\n",
    "\n",
    "Shall we still try to see if a more sophisticated model can do the job? Let's pretend we have not seen anything.\n",
    "\n",
    "Now the idea is to first put all thes covariances in a data frame where things become easier to handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-passport",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_bands = {\n",
    "    \"theta\": (4.0, 8.0),\n",
    "    \"alpha\": (8.0, 15.0),\n",
    "    \"beta_low\": (15.0, 26.0),\n",
    "    \"beta_high\": (26.0, 35.0),\n",
    "}\n",
    "\n",
    "X_df = pd.DataFrame(\n",
    "  {band: list(X_covs[:, ii]) for ii, band in enumerate(frequency_bands)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pediatric-driver",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-panic",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-profit",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respiratory-engineering",
   "metadata": {},
   "source": [
    "Covariances in a Data Frame!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-trial",
   "metadata": {},
   "outputs": [],
   "source": [
    " y = train_cases.Age.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-height",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV, GammaRegressor\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import coffeine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distant-reservoir",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filter_bank_transformer = coffeine.make_filter_bank_transformer(\n",
    "    names=['theta', 'alpha', 'beta_low', 'beta_high'], method='riemann',\n",
    "    projection_params=dict(n_compo=40))\n",
    "\n",
    "\n",
    "filter_bank_model = make_pipeline(filter_bank_transformer, StandardScaler(),\n",
    "                                  RidgeCV(alphas=np.logspace(-3, 10, 100)))\n",
    "\n",
    "y_pred = cross_val_predict(estimator=filter_bank_model, X=X_df, y=y)\n",
    "plt.scatter(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-failure",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "print(r2_score(y_true=y, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passing-baptist",
   "metadata": {},
   "source": [
    "From a theoretical standpoint and empirically, this type of model is expected to work better. We can think a moment together about it. It is most likely that something is seriously wrong about the way we are using and processing the data, which can be very tricky when first encountering a new curated dataset. Something to be clarified in the nearer future; **But we're working on it together with the CHBM team**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-radius",
   "metadata": {},
   "source": [
    "But it does not mean we have to stop here. The coarse patterning in the covariance matrix susggest a dominant low-rank structure. We can explore that further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-waste",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import svd\n",
    "s_values = np.sqrt(np.array([svd(cc[1], compute_uv=False) for cc in X_covs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-responsibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(s_values.T);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-filter",
   "metadata": {},
   "source": [
    "We now see that a few directions of variance dominate the spectrum. And one subject seems to even have flat spectrum. **Is it the same subject that we exculded prviously**? And does it matter?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-tuition",
   "metadata": {},
   "source": [
    "This together with the bad result obtained with the Riemannian embedding suggests that there is some model violation. We should explore more optinos in coffeine's filter bank model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('base': conda)",
   "language": "python",
   "name": "python39164bitbasecondae0fabca903dd46b1afc808730ddf177a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
